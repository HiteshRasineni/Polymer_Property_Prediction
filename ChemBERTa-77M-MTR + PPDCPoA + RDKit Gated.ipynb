{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12984394,"sourceType":"datasetVersion","datasetId":8218393},{"sourceId":439070,"sourceType":"modelInstanceVersion","modelInstanceId":358191,"modelId":379520}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#sep-10\n# ============================\n# NeurIPS 2025: ChemBERTa-77M-MTR + PPDCPoA + RDKit Gated Fusion\n# ============================\nimport sys, subprocess, os\nwheel_dir=\"/kaggle/input/rdkit-wheels/rdkit_wheels\"\n\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\", \n    \"--no-index\", \"--find-links=\" + wheel_dir, \n    \"rdkit-pypi==2022.9.5\", \n    \"xgboost\", \"lightgbm\", \"catboost\",\n    \"numpy\", \"pandas\", \"scikit-learn\", \"tqdm\", \"optuna\",\n    \"transformers\"\n])\n\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\n\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nfrom transformers import AutoTokenizer, AutoModel\n\n# ============================\n# 0. Reproducibility & device\n# ============================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMAX_LEN = 128\nBATCH_SIZE = 16\nEPOCHS = 100\n\n# Best Optuna params for RNN+RDKit\nLR = 0.00095\nBEST_EMBED_DIM = 128\nBEST_RNN_HIDDEN = 64\nBEST_RDKIT_HIDDEN = 128\nBEST_DROPOUT = 0.164\n\n# ============================\n# 1. Load Data\n# ============================\ntrain = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\")\n\nsupp_tg  = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\")\nsupp_ffv = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\")\nsupp_tc  = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\")\n\ntargets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n\nfor col in targets:\n    if col != \"Tg\": supp_tg[col] = np.nan\nfor col in targets:\n    if col != \"FFV\": supp_ffv[col] = np.nan\nsupp_tc = supp_tc.rename(columns={\"TC_mean\": \"Tc\"})\nfor col in targets:\n    if col != \"Tc\": supp_tc[col] = np.nan\n\ntrain_full = pd.concat([train, supp_tg, supp_ffv, supp_tc], ignore_index=True)\n\n# ============================\n# 2. Tokenizers & SMILES Encoding\n# ============================\n# ChemBERTa-MTR\nchemberta_path = \"/kaggle/input/chemberta-77m-mtr/pytorch/default/1/ChemBERTa-77M-MTR\"\nchem_tokenizer = AutoTokenizer.from_pretrained(chemberta_path)\n\n# RNN SMILES tokenizer\nall_smiles = pd.concat([train_full[\"SMILES\"], test[\"SMILES\"]])\nvocab = sorted(set(\"\".join(all_smiles.values)))\nstoi = {ch: i+1 for i,ch in enumerate(vocab)}  # 0 reserved\ndef encode_smiles(s, max_len=MAX_LEN):\n    tokens = [stoi.get(ch,0) for ch in s[:max_len]]\n    return tokens + [0]*(max_len-len(tokens))\n\ntrain_full[\"encoded\"] = train_full[\"SMILES\"].apply(encode_smiles)\ntest[\"encoded\"] = test[\"SMILES\"].apply(encode_smiles)\n\n# ============================\n# 3. RDKit Descriptors\n# ============================\ndef calc_rdkit_feats(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return np.zeros(5)\n    return np.array([\n        Descriptors.MolWt(mol),\n        Descriptors.MolLogP(mol),\n        Descriptors.TPSA(mol),\n        Descriptors.NumHDonors(mol),\n        Descriptors.NumHAcceptors(mol)\n    ])\n\ntrain_rdkit = np.vstack(train_full[\"SMILES\"].apply(calc_rdkit_feats).values)\ntest_rdkit  = np.vstack(test[\"SMILES\"].apply(calc_rdkit_feats).values)\n\nscaler_rdkit = StandardScaler()\ntrain_rdkit = scaler_rdkit.fit_transform(train_rdkit)\ntest_rdkit  = scaler_rdkit.transform(test_rdkit)\n\n# ============================\n# 4. Scale Targets\n# ============================\nscalers = {}\nfor col in targets:\n    scaler = StandardScaler()\n    vals = train_full[col].dropna().values.reshape(-1,1)\n    scaler.fit(vals)\n    scalers[col] = scaler\n    train_full[col] = train_full[col].apply(\n        lambda v: scaler.transform([[v]])[0][0] if pd.notna(v) else np.nan\n    )\n\n# ============================\n# 5. Dataset\n# ============================\nclass FusionDataset(Dataset):\n    def __init__(self, df, rdkit_feats, targets=None):\n        self.smiles_rnn = np.stack(df[\"encoded\"].values)\n        self.smiles_bert = df[\"SMILES\"].tolist()\n        self.rd = rdkit_feats\n        self.targets = df[targets].values if targets is not None else None\n\n    def __len__(self): return len(self.smiles_rnn)\n\n    def __getitem__(self, idx):\n        # RNN input\n        x_rnn = torch.tensor(self.smiles_rnn[idx], dtype=torch.long)\n        # RDKit features\n        rd = torch.tensor(self.rd[idx], dtype=torch.float)\n        # ChemBERTa encoding\n        enc = chem_tokenizer(\n            self.smiles_bert[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN,\n            return_tensors=\"pt\"\n        )\n        input_ids = enc[\"input_ids\"].squeeze(0)\n        attention_mask = enc[\"attention_mask\"].squeeze(0)\n\n        if self.targets is not None:\n            y = torch.tensor(self.targets[idx], dtype=torch.float)\n            return x_rnn, rd, input_ids, attention_mask, y\n        return x_rnn, rd, input_ids, attention_mask\n\ntrain_dataset = FusionDataset(train_full, train_rdkit, targets=targets)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataset = FusionDataset(test, test_rdkit, targets=None)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# ============================\n# 6. Model (ChemBERTa-MTR + RNN + RDKit Gated Fusion)\n# ============================\nclass FusionModel(nn.Module):\n    def __init__(self, vocab_size, rdkit_dim, embed_dim, rnn_hidden, rdkit_hidden, dropout, num_targets=5):\n        super().__init__()\n        # ChemBERTa-MTR\n        self.bert = AutoModel.from_pretrained(chemberta_path)\n\n        # SMILES RNN\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=5, padding=2)\n        self.gru = nn.GRU(embed_dim, rnn_hidden, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(rnn_hidden*2,1)\n\n        # RDKit\n        self.rdkit_mlp = nn.Sequential(\n            nn.Linear(rdkit_dim, rdkit_hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(rdkit_hidden, rdkit_hidden),\n            nn.ReLU()\n        )\n        self.rd_proj = nn.Linear(rdkit_hidden, rnn_hidden*2)\n\n        # Gate fusion\n        self.gate_c = nn.Linear(rnn_hidden*2 + self.bert.config.hidden_size, 128, bias=False)\n        self.gate_r = nn.Linear(rdkit_hidden, 128, bias=False)\n        self.gate_out = nn.Linear(128, rnn_hidden*2)\n        nn.init.constant_(self.gate_out.bias, -1.0)\n\n        # Final head\n        self.fc = nn.Sequential(\n            nn.Linear(rnn_hidden*2, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, num_targets)\n        )\n\n        self.rnn_hidden = rnn_hidden\n\n    def forward(self, x_rnn, rd, input_ids, attention_mask):\n        # RNN path\n        x = self.embedding(x_rnn)\n        x = x.permute(0,2,1)\n        x = torch.relu(self.conv(x))\n        x = x.permute(0,2,1)\n        out, _ = self.gru(x)\n        attn = torch.softmax(self.attention(out), dim=1)\n        context_rnn = torch.sum(attn*out, dim=1)\n\n        # ChemBERTa path\n        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        context_bert = bert_out.last_hidden_state[:,0,:]\n\n        # Concatenate RNN + BERT\n        context = torch.cat([context_rnn, context_bert], dim=1)\n\n        # RDKit path\n        rd_repr = self.rdkit_mlp(rd)\n        rd_proj = self.rd_proj(rd_repr)\n\n        # Gating\n        g = F.relu(self.gate_c(context) + self.gate_r(rd_repr))\n        gate = torch.sigmoid(self.gate_out(g))\n\n        fused = context_rnn * (1.0 - gate) + rd_proj * gate\n        preds = self.fc(fused)\n        return preds\n\nmodel = FusionModel(\n    vocab_size=len(stoi)+1,\n    rdkit_dim=train_rdkit.shape[1],\n    embed_dim=BEST_EMBED_DIM,\n    rnn_hidden=BEST_RNN_HIDDEN,\n    rdkit_hidden=BEST_RDKIT_HIDDEN,\n    dropout=BEST_DROPOUT,\n    num_targets=len(targets)\n).to(DEVICE)\n\n# ============================\n# 7. Training\n# ============================\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\ndef masked_mse_loss(preds, labels):\n    mask = ~torch.isnan(labels)\n    return ((preds - labels)[mask]**2).mean()\n\nbest_mae = float(\"inf\")\nbest_model_path = \"best_model.pt\"\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    for X, rd, input_ids, attention_mask, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n        X, rd, input_ids, attention_mask, y = X.to(DEVICE), rd.to(DEVICE), input_ids.to(DEVICE), attention_mask.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(X, rd, input_ids, attention_mask)\n        loss = masked_mse_loss(preds, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_loss = total_loss/len(train_loader)\n\n    # Evaluation\n    model.eval()\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for X, rd, input_ids, attention_mask, y in DataLoader(train_dataset, batch_size=BATCH_SIZE):\n            X, rd, input_ids, attention_mask = X.to(DEVICE), rd.to(DEVICE), input_ids.to(DEVICE), attention_mask.to(DEVICE)\n            preds = model(X, rd, input_ids, attention_mask).cpu().numpy()\n            all_preds.append(preds)\n            all_true.append(y.numpy())\n    all_preds = np.vstack(all_preds)\n    all_true = np.vstack(all_true)\n\n    maes = []\n    for i,col in enumerate(targets):\n        mask = ~np.isnan(all_true[:,i])\n        if mask.sum()==0: continue\n        true_vals = scalers[col].inverse_transform(all_true[mask,i].reshape(-1,1)).flatten()\n        pred_vals = scalers[col].inverse_transform(all_preds[mask,i].reshape(-1,1)).flatten()\n        mae = mean_absolute_error(true_vals, pred_vals)\n        maes.append(mae)\n        print(f\"Epoch {epoch+1} | {col}: MAE={mae:.4f}\")\n    avg_mae = np.mean(maes)\n    print(f\"Epoch {epoch+1} | Avg Train Loss={avg_loss:.4f} | Avg MAE={avg_mae:.4f}\")\n\n    if avg_mae < best_mae:\n        best_mae = avg_mae\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"✅ Best model saved (epoch {epoch+1}, MAE={avg_mae:.4f})\")\n\n# ============================\n# 8. Inference\n# ============================\nprint(\"\\nLoading best model...\")\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.eval()\n\npreds_list = []\nwith torch.no_grad():\n    for X, rd, input_ids, attention_mask in test_loader:\n        X, rd, input_ids, attention_mask = X.to(DEVICE), rd.to(DEVICE), input_ids.to(DEVICE), attention_mask.to(DEVICE)\n        out = model(X, rd, input_ids, attention_mask).cpu().numpy()\n        preds_list.append(out)\npreds = np.vstack(preds_list)\n\n# Inverse transform\nfor i,col in enumerate(targets):\n    preds[:,i] = scalers[col].inverse_transform(preds[:,i].reshape(-1,1)).flatten()\n\n# ============================\n# 9. Submission\n# ============================\nsubmission = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv\")\nsubmission[targets] = preds\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ submission.csv saved\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}